





%
\documentclass[10pt,journal,compsoc]{IEEEtran}
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % The IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi


\usepackage[mathscr]{euscript}




% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx






















% NOTE: PDF hyperlink and bookmark features are not required in IEEE
%       papers and their use requires extra complexity and work.
% *** IF USING HYPERREF BE SURE AND CHANGE THE EXAMPLE PDF ***
% *** TITLE/SUBJECT/AUTHOR/KEYWORDS INFO BELOW!!           ***
\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={Bare Demo of IEEEtran.cls for Computer Society Journals},%<!CHANGE!
pdfsubject={Typesetting},%<!CHANGE!
pdfauthor={Michael D. Shell},%<!CHANGE!
pdfkeywords={Computer Society, IEEEtran, journal, LaTeX, paper,
             template}}%<^!CHANGE!






% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\title{An Efficient Data Augmentation Testing Framework for the ChestX-ray14 Dataset}




\author{Travis~Latzke}





% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Advanced Demo of IEEEtran.cls for IEEE Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.






\IEEEtitleabstractindextext{%
\begin{abstract}
This paper outlines an efficient testing framework to test how models trained on ChestX-ray14 can be improved by adding synthetic data samples to the dataset. The paper first describes how to minimize computer resources when testing on new datasets and then provide an example examples of ways to increase a dataset and evaluate how it affects a model’s performance. Lastly, the paper concludes by explaining how the framework can be extended to include any kind of image transformation for augmenting the data set and how to evaluate the dataset with any type of model.
\end{abstract}



\begin{IEEEkeywords}
Computer Society, IEEE, IEEEtran, journal, \LaTeX, paper, template.
\end{IEEEkeywords}}


\maketitle


\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc under a non-conference mode.



\IEEEpeerreviewmaketitle


\ifCLASSOPTIONcompsoc
\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
\else
\section{Introduction}
\label{sec:introduction}
\fi


\IEEEPARstart The ChestX-ray14 data set contains more than 112,000 thousand images of patient x-rays and has gained wide popularity in training state of the art classification models due to its size and breadth of different labels it has on certain diseases. Wang et al. (2017) constructed the dataset and numerous models have been trained on the dataset and have achieved state of the art results for x-ray image classification. Rajpurkar et al. (2017) had tremendous success in their research, showing that their model provided better results than trained radiologists could. Albeit their success, computer models are far from obtaining perfect results, so many research teams are constantly pooling resources together to set new benchmarks. One popular way to increase a model’s benchmark is to provide it with more training data, and data augmentation can be a good way to synthetically create more data by applying transformations on the original data set. Krizhevsky et al. (2012) had popularized the technique by applying random transformations to obtain state of the art results on the ImageNet data set. It can also be used as way to balance skewed data sets like ChestX-ray14.

\section{Framework}
\subsection{Mathematical Overview}
Let $M$ be a binary classification model and let $O$ be the original ChestX-ray 14 data set. To train M, O is split into $O_{train}$ (70\%), $O_{validation}$ (10\%), and $O_{test}$ (20\%) and optimize the binary cross entropy loss with weights:
\\
\\
$L(X, y) = -w_+ * y$log$p(Y = 1|X)$
\\.\hspace{1cm} \ \ \ \ \ $-w_- * y$log$p(Y = 1|X)$
\\
\\
Where $p(Y = i|X)$ is the probability that the network assigns to the label $i, w_+ = |N|/(|P|+|N|)$, and $w_- = |P|/(|P|+|N|)$ with $|P|$ and $|N|$ the number of positive cases and negative cases of pneumonia in the training set respectively.
\\
\\
After each training epoch, the AUCROC metric is calculated on the validation set and is denoted by $M(O_{validation})$. Training halts after the AUCROC stops improving for five consecutive epochs. $M$'s parameters are saved for $max(O_{validation})$
\\
\\
Finally, the metric that is calculated on the test set: $M(O_{test})$ will be referred to as the baseline metric because it is derived purely from the original dataset.
\\
\\
To increase the performance of $M$, the concept of data augmentation will be introduced. First, let t be a transformation function that takes input $o \in O_{train}$ where the label of $o$ is positive. The transformation is constrained to positive images only because the dataset is heavily skewed in favor of data samples with a negative label.
\\ \\
$g = t(o)$, is a generated image under the transformation $t$ and maintains the same label as $o$. The definition of a transformation makes it possible to generate new datasets. Let $G$ be a generated dataset:
 $$G = \sum_{n=1}^{k} \cup \ t(o_i) $$
 where $o_i \in O_{train}$ and $k = size(O_{train})$
\\ \\
Finally, an augmented dataset $A$ can be constructed by adding the Generated dataset $G$ to the original training set $O_{train}$.
\\
$$A = G \cup O_{train}$$
\\
To validate if the augmented dataset $A$ contains new data samples that improve a model's performance, a new model $M$' is trained similar to the first model $M$, except the weights are adjusted to account for the new data samples in $A$. \\ \\
 $w_+ = |N|/(|P|+|N|)$\\
 $w_- = |2*P|/(|2*P|+|N|)$\\ \\ \\

If $M$'$(O_{test}) > M(O_{test})$, then the augmented data set increases then models performance and if $M$'$(O_{test}) < M(O_{test})$ then it can be concluded that training a model with stronger weights and the original data is better for the model's performance.
%
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
% \IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
% for IEEE Computer Society journal papers produced under \LaTeX\ using
% IEEEtran.cls version 1.8b and later.
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)
% I wish you the best of success.

% \hfill mds

% \hfill August 26, 2015

\subsection{Computer Resource Efficiency}
To avoid the extra overhead in memory for ChestX-ray14, a custom data loader was implemented to perform the data augmentation in line with the training step instead of as a pre-processing step. The data loader is the central piece for efficient data augmentation on this training set. It loads data in batches like a traditional data loader, but it also spawns a new thread to perform data augmentation on the current data batch. It essentially creates new data while the original data is being processed in the model training step. This minimizes the extra time needed to transform the data into new synthetic samples. The data loader is flexible enough to handle a list of different image transformations used for the data augmentation.  To perform an analysis on performance of this framework, three different data transformations are used for data augmentation for two different types of convolutional neural networks. The neural networks and transformations used in this analysis served for examples in this framework. The framework is actually model and transformation invariant; thus, researchers can easily extend this framework with models and transformations of their own flavor. Below is a brief description of the different transformations and models used to test the performance of this type of framework. It is a good practice to identify how a specific transform can be of make the model invariant to certain types of features, bearing in mind that the transformation might add features that hurt the model’s performance. Contemplating the effects before blindly running a random transformation on a dataset of this size could save researchers’ time.  Therefore, the following specifies three transforms followed by a hypothesis on how each could affect the model’s performance.



%\subsubsection{Subsubsection Heading Here}
%Subsubsection text here.



\section{A Simple Example}
The conclusion goes here.

\subsection{Architecture}
\subsection{Data Analysis}
% training graphs
% aucroc metric table
% stats on resources

\section{Conclusion}
The table above gives an intuitive hypothesis about how usefulness of each transformation.  For example, the salt and pepper transformation helped improve the AUROC metric for each of the CNN's in this study; it's likely that the augmented data helps generalize the model's ability to make a correct classification. On the other hand, augmented data that resulted from reflection appears to have lowered the AUROC for each of the models, so one can conclude that it's better to assign a higher penalty to the positive labels than to include the augmented data in the training set. \\ \\
This table can easily be extended to include new transformations and to check if the transformation is useful compared to the base AUROC value. If the new value is lower than the base value, then it is likely that the new model is suffering from overfitting and the new data is not adding any value to the dataset. Because the training time is significantly less, it is suggested to first test out a new image transformation on the LeNet architecture. If the augmented data causes overfitting on LeNet, it’s likely that it won’t be useful for other models. Knowing this could save researchers hours, or even days of time, waiting on their models to be trained.

\section{Code}
The code for this project is available at {insert link here when available} and there are instructions on how to run it and achieve the same results specified in the table above. Also, the code is flexible enough for researchers to test out a new transformation and check whether the new augmented data is useful in future studies.




% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi





% references section

\begin{thebibliography}{1}


\bibitem{IEEEhowto:kopka}
Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, et al. Chexnet: Radiologistlevel pneumonia detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225, 2017.

\bibitem{IEEEhowto:kopka}
Wang, Xiaosong, Peng, Yifan, Lu, Le, Lu, Zhiyong, Bagheri, Mohammadhadi, and Summers, Ronald M. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. arXiv preprint arXiv:1705.02315, 2017.

\bibitem{IEEEhowto:kopka}
Huang, Gao, Liu, Zhuang, Weinberger, Kilian Q, and van der Maaten, Laurens. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016.

\bibitem{IEEEhowto:kopka}
Krizhevsky, A., Sutskever, I., Hinton, G.E., "Imagenet classification with deep convolutional neural networks," In: Advances in neural information processing systems, 1097-1105, (2012).

\bibitem{IEEEhowto:kopka}
Y. LeCun, K. Kavukcuoglu, and C. Farabet, "Convolutional networks and applications in vision," Proceedings of IEEE International Symposium on Circuits and Systems (ISCAS), pp. 253-256, 2010.

\bibitem{IEEEhowto:kopka}
Y. LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, volume 2, pages II–97. IEEE, 2004.

\bibitem{IEEEhowto:kopka}
Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, 1990.

\bibitem{IEEEhowto:kopka}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015.

\end{thebibliography}




%[1] Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, et al. Chexnet: Radiologistlevel pneumonia detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225, 2017.


% that's all folks
\end{document}


